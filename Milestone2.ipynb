{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded4e5cb-619f-4cd1-8337-564420e68719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains all of the imports necessary for data exploration\n",
    "\n",
    "#First pip install all libraries\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "\n",
    "#Clone the repository\n",
    "!git clone https://github.com/dregmi08/Milestone-2-Data-Exploration-Initial-Preprocessing.git\n",
    "\n",
    "#Import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#Create new virual space so we can install spacy\n",
    "!python -m venv spacy_env\n",
    "!source spacy_env/bin/activate\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "# Import the Spotify dataset using pandas\n",
    "spotify_df = pd.read_csv('Milestone-2-Data-Exploration-Initial-Preprocessing/reviews.csv')\n",
    "\n",
    "# Printing shape of the spotify dataframe, this tells us that we have 61,594 observations and 5 features\n",
    "print(\"Printing shape upon reading from the dataframe\", spotify_df.shape)\n",
    "\n",
    "# Printing what features we have in our dataset\n",
    "print(\"Printing all the unique features:\", spotify_df.columns)\n",
    "\n",
    "# Print summary of data\n",
    "print(\"Printing description about the numerical features:\", spotify_df.describe())\n",
    "\n",
    "# See if we have any null values/missing data \n",
    "# First see which columns have any null values\n",
    "print(spotify_df.isnull().any())\n",
    "\n",
    "# Since we see that the reply column does have null data, let's see how many null values exist in the reply column\n",
    "print(\"Printing number of null entries for the Reply feature:\", spotify_df['Reply'].isnull().sum())\n",
    "\n",
    "# We have a ton of null data in the reply category (61378/61594 of the observations have a null value for the reply)\n",
    "# We are dropping the reply feature from the dataset for the following reasons:\n",
    "# \n",
    "# 1. Limited Contribution: Only 216 of 61,594 observations contain non-null values for this feature, meaning 99.6% of observations are null. \n",
    "#    With such a high degree of missingness, this feature is unlikely to add significant value to our model, and removing it will streamline our dataset.\n",
    "# \n",
    "# 2. Textual Data and Noise Risk: Filling in missing data for 61k+ observations would introduce excessive noise, particularly because the feature \n",
    "#    contains text, which is challenging to impute meaningfully without bias.\n",
    "# \n",
    "# 3. Low Relevance: The replies largely consist of generic comments from the Spotify team asking for improvement ideas, information which is already \n",
    "#    covered in the original review feature. As a result, this feature is redundant and does not enhance the dataset.\n",
    "\n",
    "# Drop the feature\n",
    "spotify_df = spotify_df.drop(columns = ['Reply'])\n",
    "\n",
    "# Sanity check number of features and shape after dropping\n",
    "print(\"Columns after dropping reply column:\", spotify_df.columns)\n",
    "print(\"Shape of data frame after dropping reply column:\", spotify_df.shape)\n",
    "\n",
    "# We are also dropping the Time_submitted column of the dataset:\n",
    "# \n",
    "# 1. The Time_submitted column does not contribute much to what we're trying to find. Our project focuses on classifying reviews in \n",
    "# categories: \"Positive\", \"Negative\"(we are definitely doing these categories), \"Slightly Positive\", \"Slightly Negative\", \"Neutral\"\n",
    "# (we might also classify reviews into these categories) and ranking what features/problems can be improved about Spotify from most important\n",
    "# (most frequently complained about) to least important. The time a review was submitted doesn't give us any relevant information here,\n",
    "# so we're going to drop it.\n",
    "# \n",
    "\n",
    "# Drop the Time_submitted\n",
    "spotify_df = spotify_df.drop(columns = ['Time_submitted'])\n",
    "\n",
    "# Sanity check number of features and shape after dropping\n",
    "print(\"Columns after dropping time submitted column:\", spotify_df.columns)\n",
    "print(\"Shape of data frame after dropping time submitted column:\", spotify_df.shape)\n",
    "\n",
    "# In conclusion, we are keeping three features in our dataset, Review (the text review a user submits), Total_thumbsup (the number of\n",
    "# thumbs up a review received), 'Rating', number of stars the user gave along with the review. The Rating and the Total_thumbsup are both\n",
    "# numerical data, and the Review is textual\n",
    "\n",
    "# OVERVIEW: To plot our numerical data, we are going to plot two things: \n",
    "    # 1. The number of thumbs up reviews get for each star rating\n",
    "    # 2. The number of reviews corresponding to each rating\n",
    "# The goal here is just to understand what the distributions are: Do we have equal distributions among all star ratings? Or do we have a bimodal distribution with peaks at 1 and 5 stars?\n",
    "# How many people agree with each star rating based on the thumbs up?\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Get the thumbs up for each rating\n",
    "thumbs_up_by_rating = spotify_df.groupby('Rating')['Total_thumbsup'].sum().reset_index()\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Bar plot for total thumbs up by star rating\n",
    "sns.barplot(data=thumbs_up_by_rating, x='Rating', y='Total_thumbsup', palette='viridis', hue = 'Rating')\n",
    "\n",
    "# Set plot titles and labels\n",
    "plt.title('Total Thumbs Up for Each Star Rating')\n",
    "plt.xlabel('Star Rating')\n",
    "plt.ylabel('Total Thumbs Up')\n",
    "plt.xticks(rotation=0) \n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Plot for Ratings\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Calculate the frequency of each rating\n",
    "rating_counts = spotify_df['Rating'].value_counts().reset_index()\n",
    "rating_counts.columns = ['Rating', 'Frequency'] \n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=rating_counts, x ='Rating', y ='Frequency', palette = 'viridis', hue = 'Rating')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Frequency of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# OVERVIEW: What we are going to essentially be doing in this code block is to find the top 20 words by frequency in each rating. Users who left reviews left a textual review as well as a star rating\n",
    "# from 1-5 stars, and we are going to split all of our reviews into 1, 2, 3, 4, and 5 stars. From here, we are going to find the top 20 words that appear in reviews the most for each star rating\n",
    "# Before we calculate the top 20 words, we need to filter out words that don't add much value to reviews, these are called stop words, and examples of these are 'a','an', 'the', 'in', 'on', 'at', etc.\n",
    "\n",
    "# Import spacy, an open source library for natural language processing\n",
    "import spacy\n",
    "\n",
    "# Separate reviews by ratings\n",
    "one_star_reviews = spotify_df.loc[spotify_df['Rating'] == 1, 'Review']\n",
    "two_star_reviews = spotify_df.loc[spotify_df['Rating'] == 2, 'Review']\n",
    "three_star_reviews = spotify_df.loc[spotify_df['Rating'] == 3, 'Review']\n",
    "four_star_reviews = spotify_df.loc[spotify_df['Rating'] == 4, 'Review']\n",
    "five_star_reviews = spotify_df.loc[spotify_df['Rating'] == 5, 'Review']\n",
    "\n",
    "# Combine reviews into a single string for each rating\n",
    "reviews_strings = [\n",
    "    ' '.join(one_star_reviews),\n",
    "    ' '.join(two_star_reviews),\n",
    "    ' '.join(three_star_reviews),\n",
    "    ' '.join(four_star_reviews),\n",
    "    ' '.join(five_star_reviews)\n",
    "]\n",
    "\n",
    "# Load the English model in SpaCy and configure without parser or NER as we don't need them for what we're doing\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Function to split text into manageable chunks (spacy will complain if you don't split the data into manageable chunks because our dataset is large)\n",
    "# this function processes the text chunk that you pass it \n",
    "def chunk_text(text, chunk_size = 500_000):\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Tokenize and filter each review string by chunking, this array will keep track of the tokenized reviews for each star\n",
    "filtered_tokenized_reviews = []\n",
    "\n",
    "# For each review string\n",
    "for review_string in reviews_strings:\n",
    "    \n",
    "    # local var to keep track of the tokens we want to keep so far\n",
    "    chunked_tokens = []\n",
    "\n",
    "    # for each chunk in the review string\n",
    "    for chunk in chunk_text(review_string.lower()):\n",
    "        \n",
    "        # process the chunk\n",
    "        doc = nlp(chunk)\n",
    "\n",
    "        # we are going to be appending multiple tokens(words) to the chunked token array\n",
    "        chunked_tokens.extend(\n",
    "            #for every token in the doc\n",
    "            token.text for token in doc\n",
    "\n",
    "            # if the token not a stop word and no a punctuation mark, then we will append it to our chunked tokens array\n",
    "            if not token.is_stop and not token.is_punct\n",
    "        )\n",
    "\n",
    "    # append the tokens for every single chunk in the review string to our 'filtered_tokenized_reviews' array\n",
    "    filtered_tokenized_reviews.append(chunked_tokens)\n",
    "\n",
    "# Calculate word frequencies for each rating's reviews\n",
    "word_counts_by_rating = [Counter(tokens) for tokens in filtered_tokenized_reviews]\n",
    "\n",
    "# Define a dictionary that will store the top 20 words by frequency for every rating\n",
    "top20_per_rating = {}\n",
    "\n",
    "# Map the string \"*number* star\" to the 20 most common words for that rating, do this for every rating\n",
    "for i, word_count in enumerate(word_counts_by_rating, start=1):\n",
    "    top20_per_rating[f\"{i}-star\"] = word_count.most_common(20)\n",
    "\n",
    "# Create separate plots for each rating\n",
    "for rating, words in top20_per_rating.items():\n",
    "    # Create a DataFrame for the current rating\n",
    "    df = pd.DataFrame(words, columns=['Word', 'Frequency'])\n",
    "    \n",
    "    # Create a bar plot to plot word frequency for 1-5 stars\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.barplot(data=df, x='Frequency', y='Word', palette='muted', hue='Word')\n",
    "    plt.title(f'Top 20 Words for {rating} Reviews')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Word')\n",
    "    plt.xlim(0, df['Frequency'].max() + 1000)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
